{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"bert_training_twitter.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNBxx+N5PWb7qNDP+NB7iwK"},"kernelspec":{"name":"python3","display_name":"Python 3"},"widgets":{"application/vnd.jupyter.widget-state+json":{"23d051e8a8b14a808a54ee44fe758a38":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_523edb5d5278467181f5a95905e09996","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_f6fde74f88884bcc998a124f9dbb32e9","IPY_MODEL_22820081c10d41a589377a95fe69fbb8"]}},"523edb5d5278467181f5a95905e09996":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"f6fde74f88884bcc998a124f9dbb32e9":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_a17c16916e9343479c98545db9106c86","_dom_classes":[],"description":"100%","_model_name":"FloatProgressModel","bar_style":"success","max":800000,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":800000,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_7d1cb2052f4d410b9c055fdeec3ca518"}},"22820081c10d41a589377a95fe69fbb8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_79528c20de904dd5bc2eb1665eb3944e","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 800000/800000 [00:08&lt;00:00, 94466.14it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_40b120849f5a41d09f39af69084a6487"}},"a17c16916e9343479c98545db9106c86":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"7d1cb2052f4d410b9c055fdeec3ca518":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"79528c20de904dd5bc2eb1665eb3944e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"40b120849f5a41d09f39af69084a6487":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"86c3b69775034893a0d46307f72c33e3":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_b6ebfcb2de57451b80a30ad5b56459f4","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_6b7c8ab7f76d41ccaa0f4695f5be5cc5","IPY_MODEL_eb3045a967ed4e718758c397bb6b7787"]}},"b6ebfcb2de57451b80a30ad5b56459f4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"6b7c8ab7f76d41ccaa0f4695f5be5cc5":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_2704900bf8ad458ea24d8884bc711be2","_dom_classes":[],"description":"100%","_model_name":"FloatProgressModel","bar_style":"success","max":200000,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":200000,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_4714a035b53545898266c3b0c76ec255"}},"eb3045a967ed4e718758c397bb6b7787":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_9218e2dcc1f5406c92a43d7446b5ef7e","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 200000/200000 [00:01&lt;00:00, 108945.73it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_bb6c4803e68d49ebb71c6b8dbdc4c3fc"}},"2704900bf8ad458ea24d8884bc711be2":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"4714a035b53545898266c3b0c76ec255":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"9218e2dcc1f5406c92a43d7446b5ef7e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"bb6c4803e68d49ebb71c6b8dbdc4c3fc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"1cf185bed90244ec96d559fb33274f4b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_4d1288bdec5546928349ef5d4d1c8657","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_c68e7689710441a7a262b12c40432b8d","IPY_MODEL_4056225d39a245f280634f2adb0a1bcd"]}},"4d1288bdec5546928349ef5d4d1c8657":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"c68e7689710441a7a262b12c40432b8d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_17968ac0cb4843b89841a323e0f2996c","_dom_classes":[],"description":"100%","_model_name":"FloatProgressModel","bar_style":"success","max":788716,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":788716,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_f5daae8d0259464a89743f378dd85946"}},"4056225d39a245f280634f2adb0a1bcd":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_3877fd8a0be54e8fb0b9d79a0862d80d","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 788716/788716 [05:57&lt;00:00, 2204.92it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_ff3986857564402d9eacf087e92ac887"}},"17968ac0cb4843b89841a323e0f2996c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"f5daae8d0259464a89743f378dd85946":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"3877fd8a0be54e8fb0b9d79a0862d80d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"ff3986857564402d9eacf087e92ac887":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"af39a8c916f34f14af640fd067be098d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_08942415d50d405f89131a892c5e9c2b","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_746844ede4834babb52bd9adaf550e25","IPY_MODEL_e821c3587a33431caf123c03af4edee8"]}},"08942415d50d405f89131a892c5e9c2b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"746844ede4834babb52bd9adaf550e25":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_fd84d278089d4382942a0f9992d6e32b","_dom_classes":[],"description":"100%","_model_name":"FloatProgressModel","bar_style":"success","max":197225,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":197225,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_ced3c10e9f6e4cd6924538118189d537"}},"e821c3587a33431caf123c03af4edee8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_d770780569174403b1054642dcd85a19","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 197225/197225 [01:22&lt;00:00, 2394.51it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_0c0160c7d9ba492999685d24ccf8713d"}},"fd84d278089d4382942a0f9992d6e32b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"ced3c10e9f6e4cd6924538118189d537":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"d770780569174403b1054642dcd85a19":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"0c0160c7d9ba492999685d24ccf8713d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"sDUx60kU6VoC","colab_type":"text"},"source":["First you need to install transformers and nlp package from Huggingface."]},{"cell_type":"code","metadata":{"id":"2ZHsoJBO6ZHf","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":555},"executionInfo":{"status":"ok","timestamp":1593502311046,"user_tz":-120,"elapsed":6852,"user":{"displayName":"Davide Stenner Talini","photoUrl":"","userId":"00013239745022293277"}},"outputId":"73b4e026-9ed4-402d-ee42-3efb6c1524eb"},"source":["!pip install transformers\n","!pip install nlp"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (3.0.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: tokenizers==0.8.0-rc4 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8.0rc4)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.91)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.9)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.12.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.15.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: nlp in /usr/local/lib/python3.6/dist-packages (0.3.0)\n","Requirement already satisfied: dill in /usr/local/lib/python3.6/dist-packages (from nlp) (0.3.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from nlp) (1.18.5)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.6/dist-packages (from nlp) (2.23.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from nlp) (4.41.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from nlp) (3.0.12)\n","Requirement already satisfied: pyarrow>=0.16.0 in /usr/local/lib/python3.6/dist-packages (from nlp) (0.17.1)\n","Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from nlp) (0.7)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->nlp) (2.9)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->nlp) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->nlp) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->nlp) (2020.6.20)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Nx4dqwIQ6gAm","colab_type":"text"},"source":["Now let's check which gpu colab will give to us and let's hope that is a P100 or T4."]},{"cell_type":"code","metadata":{"id":"ppUUC4ibSaAS","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":364},"executionInfo":{"status":"ok","timestamp":1593502315048,"user_tz":-120,"elapsed":2860,"user":{"displayName":"Davide Stenner Talini","photoUrl":"","userId":"00013239745022293277"}},"outputId":"0fe985f5-9a50-4e5f-baca-f0861e51425d"},"source":["!nvidia-smi"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Tue Jun 30 07:31:58 2020       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 450.36.06    Driver Version: 418.67       CUDA Version: 10.1     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   36C    P0    25W / 250W |      0MiB / 16280MiB |      0%      Default |\n","|                               |                      |                 ERR! |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"m_pmZ-JWQISo","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":52},"executionInfo":{"status":"ok","timestamp":1593502318499,"user_tz":-120,"elapsed":2867,"user":{"displayName":"Davide Stenner Talini","photoUrl":"","userId":"00013239745022293277"}},"outputId":"a98d988d-ffb5-4729-b0bf-585153dd73fe"},"source":["import transformers\n","import nlp\n","from sklearn.model_selection import train_test_split\n","import pandas as pd\n","import numpy as np\n","import gc\n","from transformers import BertTokenizer, BertForSequenceClassification\n","import torch\n","from tqdm.notebook import tqdm\n","from transformers import get_linear_schedule_with_warmup\n","import re\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","import random\n","import time\n","from transformers import AdamW, get_linear_schedule_with_warmup\n","import random\n","import time\n","import torch.nn as nn\n","import seaborn as sns "],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n","  import pandas.util.testing as tm\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"NcaBbWHE80II","colab_type":"code","colab":{}},"source":["def text_preprocessing(text):\n","    \"\"\"\n","    - Remove entity mentions (eg. '@united')\n","    - Correct errors (eg. '&amp;' to '&')\n","    - Remove url (eg. 'https://...', 'http://...', 'www...' to '')\n","    @param    text (str): a string to be processed.\n","    @return   text (Str): the processed string.\n","    \"\"\"\n","\n","\n","    # Remove '@name'\n","    text = re.sub(r'(@.*?)[\\s]', ' ', text)\n","\n","    # Replace '&amp;' with '&'\n","    text = re.sub(r'&amp;', '&', text)\n","\n","    # Remove trailing whitespace\n","    text = re.sub(r'\\s+', ' ', text).strip()\n","\n","    #remove url with http\n","    text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n","\n","    #remove url with www\n","    text = re.sub(r'www\\..*[\\r\\n]*', '', text, flags=re.MULTILINE)\n","\n","    return text\n","\n","\n","def initialize_model(epochs = 3):\n","    \"\"\"Initialize the Bert Classifier, the optimizer and the learning rate scheduler.\n","    @param epoch: (int) number of epoch to train the model\n","\n","    @return bert_classifier: (BertForSequenceClassification) bert model used for training\n","    @return optimizer:  optimizer used for training\n","    @return scheduler: (torch.optim) scheduler used for training\n","    \"\"\"\n","    # Instantiate Bert Classifier\n","    bert_classifier = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels = 2)\n","\n","    # Tell PyTorch to run the model on GPU\n","    bert_classifier.to(device)\n","\n","    # Create the optimizer\n","    optimizer = AdamW(bert_classifier.parameters(),\n","                      lr=5e-5,    # Default learning rate\n","                      eps=1e-8    # Default epsilon value\n","                      )\n","\n","    # Total number of training steps\n","    total_steps = len(train_dataloader) * epochs\n","\n","    # Set up the learning rate scheduler with warmup\n","    scheduler = get_linear_schedule_with_warmup(optimizer,\n","                                                num_warmup_steps= 0 , # Default value\n","                                                num_training_steps=total_steps)\n","    return bert_classifier, optimizer, scheduler\n","\n","def set_seed(seed_value=42):\n","    \"\"\"Set seed for reproducibility.\n","    @param seed: (int) seed used in different random number generator.\n","    \"\"\"\n","    random.seed(seed_value)\n","    np.random.seed(seed_value)\n","    torch.manual_seed(seed_value)\n","    torch.cuda.manual_seed_all(seed_value)\n","\n","def train(model, optimizer, scheduler, train_dataloader, val_dataloader=None, epochs = 3, evaluation=False):\n","    \"\"\"Train the BertClassifier model.\n","    @param model: (BertForSequenceClassification)\n","    @param optimizer: optimizer used for training\n","    @param scheduler: (torch.optim) scheduler used for training\n","    @param train_dataloader: (torch.utils.data.dataloader.DataLoader)\n","    @param val_dataloader: (torch.utils.data.dataloader.DataLoader)\n","    @param epoch: (int) number of epoch\n","    @param evaluation: (Bool) parameter to choose if evaluate or not at the end of epoch.\n","    \"\"\"\n","    # Start training loop\n","    print(\"Start training...\\n\")\n","    \n","    for epoch_i in range(epochs):\n","        # =======================================\n","        #               Training\n","        # =======================================\n","        # Print the header of the result table\n","        print(f\"{'Epoch':^7} | {'Batch':^7} | {'Train Loss':^12} | {'Val Loss':^10} | {'Val Acc':^9} | {'Elapsed':^9}\")\n","        print(\"-\"*70)\n","\n","        # Measure the elapsed time of each epoch\n","        t0_epoch, t0_batch = time.time(), time.time()\n","\n","        # Reset tracking variables at the beginning of each epoch\n","        total_loss, batch_loss, batch_counts = 0, 0, 0\n","\n","        # Put the model into the training mode\n","        model.train()\n","\n","        # For each batch of training data...\n","        for step, batch in enumerate(train_dataloader):\n","            batch_counts +=1\n","            # Load batch to GPU\n","            b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n","\n","            # Zero out any previously calculated gradients\n","            model.zero_grad()\n","\n","            # Perform a forward pass. This will return logits.\n","            logits = model(b_input_ids, b_attn_mask)[0]\n","            \n","            # Compute loss and accumulate the loss values\n","            loss = loss_fn(logits, b_labels)\n","            batch_loss += loss.item()\n","            total_loss += loss.item()\n","\n","            # Perform a backward pass to calculate gradients\n","            loss.backward()\n","\n","            # Clip the norm of the gradients to 1.0 to prevent \"exploding gradients\"\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","\n","            # Update parameters and the learning rate\n","            optimizer.step()\n","            scheduler.step()\n","\n","            # Print the loss values and time elapsed for every 20 batches\n","            if (step % 1000 == 0 and step != 0) or (step == len(train_dataloader) - 1):\n","                # Calculate time elapsed for 20 batches\n","                time_elapsed = time.time() - t0_batch\n","\n","                # Print training results\n","                print(f\"{epoch_i + 1:^7} | {step:^7} | {batch_loss / batch_counts:^12.6f} | {'-':^10} | {'-':^9} | {time_elapsed:^9.2f}\")\n","\n","                # Reset batch tracking variables\n","                batch_loss, batch_counts = 0, 0\n","                t0_batch = time.time()\n","\n","        # Calculate the average loss over the entire training data\n","        avg_train_loss = total_loss / len(train_dataloader)\n","\n","        print(\"-\"*70)\n","        # =======================================\n","        #               Evaluation\n","        # =======================================\n","        if evaluation == True:\n","            # After the completion of each training epoch, measure the model's performance\n","            # on our validation set.\n","            val_loss, val_accuracy = evaluate(model, val_dataloader)\n","\n","            # Print performance over the entire training data\n","            time_elapsed = time.time() - t0_epoch\n","            \n","            print(f\"{epoch_i + 1:^7} | {'-':^7} | {avg_train_loss:^12.6f} | {val_loss:^10.6f} | {val_accuracy:^9.2f} | {time_elapsed:^9.2f}\")\n","            print(\"-\"*70)\n","        print(\"\\n\")\n","    \n","    print(\"Training complete!\")\n","\n","\n","def evaluate(model, val_dataloader):\n","    \"\"\"After the completion of each training epoch, measure the model's performance\n","    on our validation set.\n","\n","    @param model: (BertForSequenceClassification) model to evaluate\n","    @param val_dataloader: (torch.utils.data.dataloader.DataLoader) dataloader of validation set\n","\n","    @return val_los: (np.float) Validation loss calculated on the model applied on validation set.\n","    @return val_accuracy: (np.float) Validation Accuracy calculated on the model applied on validation set.\n","    \"\"\"\n","    # Put the model into the evaluation mode. The dropout layers are disabled during\n","    # the test time.\n","    model.eval()\n","\n","    # Tracking variables\n","    val_accuracy = []\n","    val_loss = []\n","\n","    # For each batch in our validation set...\n","    for batch in val_dataloader:\n","        # Load batch to GPU\n","        b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n","\n","        # Compute logits\n","        with torch.no_grad():\n","            logits = model(b_input_ids, b_attn_mask)[0]\n","\n","        # Compute loss\n","        loss = loss_fn(logits, b_labels)\n","        val_loss.append(loss.item())\n","\n","        # Get the predictions\n","        preds = torch.argmax(logits, dim=1).flatten()\n","\n","        # Calculate the accuracy rate\n","        accuracy = (preds == b_labels).cpu().numpy().mean() * 100\n","        val_accuracy.append(accuracy)\n","\n","    # Compute the average accuracy and loss over the validation set.\n","    val_loss = np.mean(val_loss)\n","    val_accuracy = np.mean(val_accuracy)\n","\n","    return val_loss, val_accuracy\n","\n","def preprocessing_for_bert(data, tokenizer):\n","    \"\"\"Perform required preprocessing steps for pretrained BERT.\n","    @param    data (np.array): Array of texts to be processed.\n","    @param    tokenizer(transformers.tokenization_bert): Tokenizer used to tokenize the sentence\n","\n","    @return   input_ids (torch.Tensor): Tensor of token ids to be fed to a model.\n","    @return   attention_masks (torch.Tensor): Tensor of indices specifying which\n","                  tokens should be attended to by the model.\n","    \"\"\"\n","    # Create empty lists to store outputs\n","    input_ids = []\n","    attention_masks = []\n","\n","    # For every sentence...\n","    for sent in tqdm(data):\n","        # `encode_plus` will:\n","        #    (1) Tokenize the sentence\n","        #    (2) Add the `[CLS]` and `[SEP]` token to the start and end\n","        #    (3) Truncate/Pad sentence to max length\n","        #    (4) Map tokens to their IDs\n","        #    (5) Create attention mask\n","        #    (6) Return a dictionary of outputs\n","\n","        #encoding sentence\n","        encoded_sent = tokenizer.encode_plus(\n","            text=sent,  # Preprocess sentence\n","            add_special_tokens=True,        # Add `[CLS]` and `[SEP]`\n","            max_length=MAX_LEN,             # Max length to truncate/pad\n","            truncation = True,              # Truncate sentence to MAX_LEN\n","            pad_to_max_length=True,         # Pad sentence to max length\n","            #return_tensors='pt',           # Return PyTorch tensor\n","            return_attention_mask=True      # Return attention mask\n","            )\n","        \n","        # Add the outputs to the lists\n","        input_ids.append(encoded_sent.get('input_ids'))\n","        attention_masks.append(encoded_sent.get('attention_mask'))\n","\n","    # Convert lists to tensors\n","    input_ids = torch.tensor(input_ids)\n","    attention_masks = torch.tensor(attention_masks)\n","\n","    return input_ids, attention_masks\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ffrCHjSe7Agd","colab_type":"text"},"source":["With NLP you can download different dataset easily.\n","We will use sentiment140 a dataset of over 1.600.000 different tweets for sentiment analysis."]},{"cell_type":"code","metadata":{"id":"0Of9AWGFQMDL","colab_type":"code","colab":{}},"source":["#We will use only train dataset because it is very large.\n","sentiment140 = nlp.load_dataset('sentiment140', split = 'train')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SmZVSEtM7uA7","colab_type":"text"},"source":["We will use half of the dataset in particular 800.000 for train and 200.000 for test."]},{"cell_type":"code","metadata":{"id":"bPDiDhhaTdFT","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":87},"executionInfo":{"status":"ok","timestamp":1593502397085,"user_tz":-120,"elapsed":75594,"user":{"displayName":"Davide Stenner Talini","photoUrl":"","userId":"00013239745022293277"}},"outputId":"693fe029-7ebe-44c7-b28c-f0b4592e1785"},"source":["#Import sentiment column to define the split\n","strat_supp = pd.DataFrame(sentiment140['sentiment']).values\n","print(f'Total number of observation: {strat_supp.shape[0]}\\n')\n","\n","#With two different split we will take the needed amount of data\n","_, supp, _strat, strat = train_test_split(pd.DataFrame(sentiment140).loc[:, ['text', 'sentiment']], strat_supp, test_size = .625, random_state = 3, shuffle = True, stratify = strat_supp)\n","train_df, test_df = train_test_split(supp, test_size = .2, random_state = 3, shuffle = True, stratify = strat)\n","\n","print(f'Train data used: {train_df.shape[0]}; Test data used: {test_df.shape[0]}')\n","\n","#free some memory\n","del _, supp, _strat, strat, strat_supp\n","gc.collect()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Total number of observation: 1600000\n","\n","Train data used: 800000; Test data used: 200000\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["22"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"markdown","metadata":{"id":"KIB8HSSO77nu","colab_type":"text"},"source":["Let's check the distribution of the length of the sentence of all the tweets."]},{"cell_type":"code","metadata":{"id":"9vDXoIBk8tXz","colab_type":"code","colab":{}},"source":["#import Bert Tokenizer with huggingface\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"foonH9TUWde4","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":300},"executionInfo":{"status":"ok","timestamp":1593502475215,"user_tz":-120,"elapsed":149558,"user":{"displayName":"Davide Stenner Talini","photoUrl":"","userId":"00013239745022293277"}},"outputId":"63ed0872-a997-49b0-90c0-99218a96999d"},"source":["supp = pd.DataFrame(nlp.load_dataset('sentiment140', split = 'train'))['text']\n","\n","#Pre-process and clean the text and apply tokenization\n","supp = [text_preprocessing(x) for x in supp]\n","supp_length = [len(x) for x in supp]\n","\n","#distribution of tweet length\n","sns.distplot(supp_length)\n","print(np.quantile([len(x) for x in supp], .99))\n","\n","del supp, supp_length\n","gc.collect()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["137.0\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["10"]},"metadata":{"tags":[]},"execution_count":8},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"YCVIP9hO8A0H","colab_type":"text"},"source":["We can see that setting max length as 150 is a good choice. In this way every tweet is truncated to 150 "]},{"cell_type":"code","metadata":{"id":"craXWDATJCcI","colab_type":"code","colab":{}},"source":["MAX_LEN = 150"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"H0rwxD9V-tFg","colab_type":"text"},"source":["Set the device as cuda if it's available otherwise cpu."]},{"cell_type":"code","metadata":{"id":"WQWfgagKToed","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":52},"executionInfo":{"status":"ok","timestamp":1593502483509,"user_tz":-120,"elapsed":911,"user":{"displayName":"Davide Stenner Talini","photoUrl":"","userId":"00013239745022293277"}},"outputId":"36e04e76-290d-49e4-a272-5d112419c5b6"},"source":["if torch.cuda.is_available():    \n","\n","    # Tell PyTorch to use the GPU.    \n","    device = torch.device(\"cuda\")\n","\n","    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n","\n","    print('We will use the GPU:', torch.cuda.get_device_name(0))\n","\n","# If not...\n","else:\n","    print('No GPU available, using the CPU instead.')\n","    device = torch.device(\"cpu\")\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["There are 1 GPU(s) available.\n","We will use the GPU: Tesla P100-PCIE-16GB\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"LoZSN6bJ-1PI","colab_type":"text"},"source":["Pre process the tweet to clean error and html part."]},{"cell_type":"code","metadata":{"id":"eZo7eN0KXBJN","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":133,"referenced_widgets":["23d051e8a8b14a808a54ee44fe758a38","523edb5d5278467181f5a95905e09996","f6fde74f88884bcc998a124f9dbb32e9","22820081c10d41a589377a95fe69fbb8","a17c16916e9343479c98545db9106c86","7d1cb2052f4d410b9c055fdeec3ca518","79528c20de904dd5bc2eb1665eb3944e","40b120849f5a41d09f39af69084a6487","86c3b69775034893a0d46307f72c33e3","b6ebfcb2de57451b80a30ad5b56459f4","6b7c8ab7f76d41ccaa0f4695f5be5cc5","eb3045a967ed4e718758c397bb6b7787","2704900bf8ad458ea24d8884bc711be2","4714a035b53545898266c3b0c76ec255","9218e2dcc1f5406c92a43d7446b5ef7e","bb6c4803e68d49ebb71c6b8dbdc4c3fc"]},"executionInfo":{"status":"ok","timestamp":1593502495828,"user_tz":-120,"elapsed":9796,"user":{"displayName":"Davide Stenner Talini","photoUrl":"","userId":"00013239745022293277"}},"outputId":"c0de67d8-bfbc-4271-bfea-22ddb12a309b"},"source":["text_train, label_train = [text_preprocessing(sent) for sent in tqdm(train_df.text)], train_df.sentiment.tolist()\n","text_test, label_test = [text_preprocessing(sent) for sent in tqdm(test_df.text)], test_df.sentiment.tolist()\n","\n","# del train, test\n","gc.collect()"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"23d051e8a8b14a808a54ee44fe758a38","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=800000.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"86c3b69775034893a0d46307f72c33e3","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=200000.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["5921"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"markdown","metadata":{"id":"4Ok9xhJp-73c","colab_type":"text"},"source":["Clean of dataset by removing empty tweet."]},{"cell_type":"code","metadata":{"id":"sgTfslMk9qx_","colab_type":"code","colab":{}},"source":["#remove blank tweet\n","remove_train = [len(x) > 0 for x in text_train]\n","text_train, label_train = list(np.array(text_train)[remove_train]), list(np.array(label_train)[remove_train])\n","\n","remove_test = [len(x) > 0 for x in text_test]\n","text_test, label_test = list(np.array(text_test)[remove_test]), list(np.array(label_test)[remove_test])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IMx4dwgO_A5E","colab_type":"text"},"source":["Set the label in correct format by switching the value 4 --> 1 for positive tweet."]},{"cell_type":"code","metadata":{"id":"fxyxJqCeZmHN","colab_type":"code","colab":{}},"source":["label_train = [1 if x > 0  else 0 for x in label_train]\n","label_test = [1 if x > 0  else 0 for x in label_test]\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"x3fXRCJ0_I8m","colab_type":"text"},"source":["Tokenization pipeline to get input and attention mask for train and test."]},{"cell_type":"code","metadata":{"id":"L0xf1hT0XaAO","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":116,"referenced_widgets":["1cf185bed90244ec96d559fb33274f4b","4d1288bdec5546928349ef5d4d1c8657","c68e7689710441a7a262b12c40432b8d","4056225d39a245f280634f2adb0a1bcd","17968ac0cb4843b89841a323e0f2996c","f5daae8d0259464a89743f378dd85946","3877fd8a0be54e8fb0b9d79a0862d80d","ff3986857564402d9eacf087e92ac887","af39a8c916f34f14af640fd067be098d","08942415d50d405f89131a892c5e9c2b","746844ede4834babb52bd9adaf550e25","e821c3587a33431caf123c03af4edee8","fd84d278089d4382942a0f9992d6e32b","ced3c10e9f6e4cd6924538118189d537","d770780569174403b1054642dcd85a19","0c0160c7d9ba492999685d24ccf8713d"]},"executionInfo":{"status":"ok","timestamp":1593503125524,"user_tz":-120,"elapsed":340750,"user":{"displayName":"Davide Stenner Talini","photoUrl":"","userId":"00013239745022293277"}},"outputId":"d8c246ca-c069-4cdf-be29-03d1bb1429fc"},"source":["train_inputs, train_masks = preprocessing_for_bert(text_train, tokenizer)\n","val_inputs, val_masks = preprocessing_for_bert(text_test, tokenizer)"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1cf185bed90244ec96d559fb33274f4b","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=788716.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"af39a8c916f34f14af640fd067be098d","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=197225.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"hjh6a5tV_PnZ","colab_type":"text"},"source":["Create tensor and dataloader for train and validation. We will use batch size as 32 as recommended by BERT's author."]},{"cell_type":"code","metadata":{"id":"wZeWnl_kf4ng","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1593504453736,"user_tz":-120,"elapsed":1130,"user":{"displayName":"Davide Stenner Talini","photoUrl":"","userId":"00013239745022293277"}},"outputId":"b860ca6c-7f3a-4cd8-bc6a-a205a8cd0c7f"},"source":["# Convert other data types to torch.Tensor\n","train_labels = torch.tensor(label_train)\n","val_labels = torch.tensor(label_test)\n","\n","# For fine-tuning BERT, the authors recommend a batch size of 16 or 32.\n","batch_size = 32\n","\n","# Create the DataLoader for our training set\n","train_data = TensorDataset(train_inputs, train_masks, train_labels)\n","train_sampler = RandomSampler(train_data)\n","train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n","\n","# Create the DataLoader for our validation set\n","val_data = TensorDataset(val_inputs, val_masks, val_labels)\n","val_sampler = SequentialSampler(val_data)\n","val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)\n","\n","gc.collect()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["406"]},"metadata":{"tags":[]},"execution_count":39}]},{"cell_type":"code","metadata":{"id":"VgdVUj5AQHrT","colab_type":"code","colab":{}},"source":["# Specify loss function\n","loss_fn = nn.CrossEntropyLoss()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"APFcNEuLOKnJ","colab_type":"text"},"source":["Load the classifier, optimizer and scheduler."]},{"cell_type":"code","metadata":{"id":"x4ZSP95DNJhO","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":124},"executionInfo":{"status":"ok","timestamp":1593504484801,"user_tz":-120,"elapsed":5143,"user":{"displayName":"Davide Stenner Talini","photoUrl":"","userId":"00013239745022293277"}},"outputId":"4f8c3ea8-1cf8-4245-b59c-5bdd2156557d"},"source":["set_seed()\n","bert_classifier, optimizer, scheduler = initialize_model(epochs = 2)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"UNQF1bPO_bZp","colab_type":"text"},"source":["Let's train the classifier for 2 epoch over the 800.000 different tweet."]},{"cell_type":"code","metadata":{"id":"Njo186JSRWLl","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1593529330571,"user_tz":-120,"elapsed":24843928,"user":{"displayName":"Davide Stenner Talini","photoUrl":"","userId":"00013239745022293277"}},"outputId":"78c0abf4-dad9-4ae1-b530-ba7af8e78c16"},"source":["train(bert_classifier, optimizer, scheduler, train_dataloader, val_dataloader, epochs = 2, evaluation = True)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Start training...\n","\n"," Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n","----------------------------------------------------------------------\n","   1    |  1000   |   0.432341   |     -      |     -     |  466.51  \n","   1    |  2000   |   0.381585   |     -      |     -     |  465.80  \n","   1    |  3000   |   0.383168   |     -      |     -     |  465.86  \n","   1    |  4000   |   0.372440   |     -      |     -     |  466.06  \n","   1    |  5000   |   0.370665   |     -      |     -     |  466.12  \n","   1    |  6000   |   0.370374   |     -      |     -     |  466.76  \n","   1    |  7000   |   0.362668   |     -      |     -     |  466.88  \n","   1    |  8000   |   0.355139   |     -      |     -     |  467.01  \n","   1    |  9000   |   0.349685   |     -      |     -     |  466.93  \n","   1    |  10000  |   0.350752   |     -      |     -     |  467.03  \n","   1    |  11000  |   0.351598   |     -      |     -     |  467.06  \n","   1    |  12000  |   0.341833   |     -      |     -     |  467.20  \n","   1    |  13000  |   0.341050   |     -      |     -     |  467.27  \n","   1    |  14000  |   0.338646   |     -      |     -     |  467.27  \n","   1    |  15000  |   0.338006   |     -      |     -     |  467.31  \n","   1    |  16000  |   0.339677   |     -      |     -     |  466.62  \n","   1    |  17000  |   0.338781   |     -      |     -     |  466.65  \n","   1    |  18000  |   0.335716   |     -      |     -     |  467.10  \n","   1    |  19000  |   0.331666   |     -      |     -     |  466.93  \n","   1    |  20000  |   0.335463   |     -      |     -     |  466.37  \n","   1    |  21000  |   0.331545   |     -      |     -     |  466.04  \n","   1    |  22000  |   0.330692   |     -      |     -     |  465.87  \n","   1    |  23000  |   0.332283   |     -      |     -     |  465.65  \n","   1    |  24000  |   0.324749   |     -      |     -     |  465.47  \n","   1    |  24647  |   0.319955   |     -      |     -     |  300.85  \n","----------------------------------------------------------------------\n","   1    |    -    |   0.350859   |  0.325876  |   85.98   | 12429.92 \n","----------------------------------------------------------------------\n","\n","\n"," Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n","----------------------------------------------------------------------\n","   2    |  1000   |   0.269736   |     -      |     -     |  466.07  \n","   2    |  2000   |   0.272061   |     -      |     -     |  465.46  \n","   2    |  3000   |   0.267427   |     -      |     -     |  465.45  \n","   2    |  4000   |   0.265563   |     -      |     -     |  465.83  \n","   2    |  5000   |   0.265998   |     -      |     -     |  465.79  \n","   2    |  6000   |   0.262943   |     -      |     -     |  465.73  \n","   2    |  7000   |   0.264798   |     -      |     -     |  465.64  \n","   2    |  8000   |   0.263992   |     -      |     -     |  465.57  \n","   2    |  9000   |   0.263501   |     -      |     -     |  465.59  \n","   2    |  10000  |   0.267067   |     -      |     -     |  465.37  \n","   2    |  11000  |   0.259958   |     -      |     -     |  465.73  \n","   2    |  12000  |   0.265353   |     -      |     -     |  465.81  \n","   2    |  13000  |   0.261596   |     -      |     -     |  465.78  \n","   2    |  14000  |   0.262003   |     -      |     -     |  465.54  \n","   2    |  15000  |   0.257035   |     -      |     -     |  465.54  \n","   2    |  16000  |   0.257543   |     -      |     -     |  465.66  \n","   2    |  17000  |   0.257251   |     -      |     -     |  466.38  \n","   2    |  18000  |   0.263378   |     -      |     -     |  466.28  \n","   2    |  19000  |   0.258527   |     -      |     -     |  465.84  \n","   2    |  20000  |   0.258641   |     -      |     -     |  466.08  \n","   2    |  21000  |   0.259510   |     -      |     -     |  466.17  \n","   2    |  22000  |   0.258792   |     -      |     -     |  466.13  \n","   2    |  23000  |   0.254609   |     -      |     -     |  465.92  \n","   2    |  24000  |   0.253049   |     -      |     -     |  465.86  \n","   2    |  24647  |   0.246809   |     -      |     -     |  301.21  \n","----------------------------------------------------------------------\n","   2    |    -    |   0.261696   |  0.326022  |   86.85   | 12412.83 \n","----------------------------------------------------------------------\n","\n","\n","Training complete!\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"6_98P0wX_iN6","colab_type":"text"},"source":["Now i will mount my google drive to save the model"]},{"cell_type":"code","metadata":{"id":"lLaHP3snsVPP","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":124},"executionInfo":{"status":"ok","timestamp":1593529364632,"user_tz":-120,"elapsed":28230,"user":{"displayName":"Davide Stenner Talini","photoUrl":"","userId":"00013239745022293277"}},"outputId":"779e8e08-51e1-4be2-9d95-7cc530f0a3dd"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"iNyQxh9X_mpO","colab_type":"text"},"source":["Save the model as suggested by huggingface."]},{"cell_type":"code","metadata":{"id":"M2Bd85pssS8M","colab_type":"code","colab":{}},"source":["from transformers.file_utils import cached_path, WEIGHTS_NAME\n","WEIGHTS_NAME\n","torch.save(bert_classifier.state_dict(), f'/content/gdrive/My Drive/Progetti Lavoro/{WEIGHTS_NAME}')"],"execution_count":null,"outputs":[]}]}